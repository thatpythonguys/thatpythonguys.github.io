[ { "title": "Group Structures", "url": "/Nov9th/", "categories": "Abstract Algebra", "tags": "subgroups, sylow, group actions", "date": "2022-11-09 08:11:00 -0600", "snippet": "I’ve been working on Beachy and Blair’s Abstract Algebra Book, specifically their chapter on Group structure (7), and I wanted to summarize what I’ve learned about group structures.Finite groups and structureThe problems and results derived from this chapter almost all concern with finite groups. We know a lot about finite groups. For example, we have this result: Theorem 1. (Burnside, 1904) Every group of order $p^a q^b$, where $p$ and $q$ are (not necessarily distinct) primes is solvable.Which was later killed by: Theorem 2 (Feit–Thompson theorem, 1963) Every odd group is solvable.I’ll explain what solvability is later, but this theorem was so fundamental that mathematicians decided it was time to classify every possible finite group (with a special condition) after this discovery was found. And they succeeded!What is the condition? The groups had to be simple.Simple groups Definition 3. A group is called simple if it has no normal subgroups except the trivial group and itself.Recall that normal subgroups are very useful. If $N$ is a normal subgroup of $G$, then we can define $G/N$, the quotient group of $G$ with $N$. It turns out the only type of subgroup that allows this structure to form is a normal subgroup.Therefore these “simple” groups have a unique property: we can’t take any quotient and get anything meaningful, we either get the whole group, or the trivial group. However, this is a good thing, this means we can’t reduce the group down.To show why this is important consider primes. We know that a prime only has itself and $1$ as divisors. Simple groups are the prime numbers of group theory. While we can’t just multiply (in group theory, we call this an external direct product) simple groups together to form a larger group, we can use a generalization of products, extensions, to get to any group given the simple groups.The easiest simple groups to describe with an undergraduate knowledge of group theory are The cyclic groups of prime order, $\\mathbb{Z}/p\\mathbb{Z}$. The alternating groups of $n$ letters, $A_n$. The projective special linear groups $\\mathrm{PSL}_n(F)$ for any finite field $F$ (with a few exceptions).I’d recommend looking into these yourself if interested.Solvable groups Definition 4. A finite group $G$ is solvable if there is a finite chain of subgroups $G = N_0\\supseteq N_1\\supseteq\\cdots \\supseteq N_n$ such that $N_i$ is a normal subgroup of $N_{i-1}$, $N_{i-1}/N_i$ is a prime cyclic group ($\\mathbb{Z}/p\\mathbb{Z}$), $N_n = \\{ 1 \\}$. Speaking very vaguely, we can mod out a normal subgroup of $G$ such that its quotient with $G$ is a prime order cyclic group, and then continue this process until we end up with the trivial group. This helps us because if $G$ is solvable, then either: It has a nontrivial subgroup, or It only has the trivial subgroup, and $G/\\{1\\}$ is a cyclic group of prime order.There are many more series like (4), which I hope to cover in a different post, but one very important one is Definition 5. A chain of subgroups for a group $G$ such that $G = N_0\\supseteq N_1\\supseteq\\cdots \\supseteq N_n$ and $N_i$ is a normal subgroup of $N_{i-1}$, $N_{i-1}/N_i$ is a simple group, $N_n = \\{ 1 \\}$, is called a composition series of $G$.The power of this series is realized with: Theorem 6 (Jordan-Hölder). All composition series for the same group have the same lengths, and the groups $N_{i-1}/N_i$, called the composition factors are just permutations of the composition factors of any other composition series.What this theorem is saying is that every composition series looks similar. The composition series lets us know what simple groups are needed to extend the trivial group to any desired group. Theorem (6) tell us that if we found a different path of extensions by simple groups, the simple groups we chose would be the same. While not every group has a composition series, the ones that do give us hints to their structure. Exercise 1. Show that $\\mathbb{Z}$ does not have a composition series.Theres an issue with these series: how do we find normal subgroups? For finite groups we have an aid.The Sylow theoremsI won’t state the Sylow Theorems here, but they allow us to guarantee the existence of subgroups of order $p^k$, where $p$ is a prime, and $k$ is the largest possible number such that $p^k$ divides the order of the group. What’s more, if we can show that there is only one of these groups, then we can conclude it is normal.There were many problems littered in the book about proving that any group of order $148$, $48$, $96$, using varying techniques, but they tend to start with finding the Sylow $p$-subgroups of a group with the given order.The proof of the Sylow Theorems relied on something with even more use, and I would argue, bridges the gap between real life and group theory.Group actions are powerfulGroup actions were the hardest thing for me to understand when I was learning group theory, so I hope to give a broad introduction to them. Definition 7. A group $G$ acts on a set $S$ by “moving” elements of $S$ in some predefined way. I write $G\\curvearrowright S$. There are 2 traits that a group action must satisfy. For $s\\in S$, and $g,h\\in G$, $1\\curvearrowright s = s$. $a\\curvearrowright (b \\curvearrowright s) = (ab) \\curvearrowright s$. We call “$\\curvearrowright$” a group action.We can define a group action however we want. Here’s the simplest example: Example 1. $(G,\\cdot)$ can act on itself. Let’s define, for $a,g \\in G$, $a\\curvearrowright g = a\\cdot g$. If we let $a$ act on all elements of $G$, it will end up permuting them in some way.The way we define the action is slightly limited, but it allows us to take full advantage of group properties. The action in Example (1) is boring, since it just uses element multiplication. For a more interesting example, we could let the $n$’th Dihedral group $D_n$ act on an $n$-gon by letting it rotate and reflect the polygon. This tells us a lot about how rotations and reflections interact on the polygon (such as $1$ rotation and $1$ reflection is the same as $n-1$ rotations), since we can abstract it to the group structure.I think group actions explain why groups are so synonymous with symmetry. You can apply a group to any object with symmetry, and then use the group structure to determine useful properties about it. Problem 2. Let $G$ be a group of order $2m$, there $m$ is odd. Show that $G$ is not simple. Proof. Consider the homomorphism $\\phi_1:G\\to S_{2m}$, which exists due to Cayley’s theorem. Let $\\phi_1(g) = f_g$, where $f_g:G\\to G$ is the permutation by $g\\curvearrowright G$. Now let $\\phi_2 : S_{2m} \\rightarrow \\{\\pm1\\}$ be the even-odd permutation homomorphism. Claim. $\\phi_2\\circ \\phi_1:G\\rightarrow \\{\\pm1\\}$ is surjective. $\\phi_2\\circ\\phi_1(1)=1$, since the identity is $(1)$, an even permutation. If $|G|=2m$, then $\\exists a\\in G$ such that $o(a)=2$.Then consider $f_a\\in S_{2m}$. For $x\\in G$, we know that $f_a(x)=y$,for some $y\\neq x\\in G$, and $f_a(f_a(x)) = x$.Therefore the transposition $(x,y)$ is in the permutation $f_a$.As we continue for all elements in $G$, we find that there are $2m/2 = m$ transpositions in $f_a$. Since $m$ is odd, $f_a$ is an odd permutation, and $\\phi_2\\circ\\phi_1(a)=-1$. Therefore $\\phi_2\\circ\\phi_1$ is surjective. Using this, we know that $\\ker(\\phi_2\\circ \\phi_1)$ is a subgroup of $G$, and\\[[G:\\ker(\\phi_2\\circ \\phi_1)] = 2,\\] so $\\ker(\\phi_2\\circ \\phi_1)\\trianglelefteq G$, and $G$ is not simple.Useful tools for examining group structureCharacteristic subgroupsCharacteristic subgroups are subgroups $H$ of a group $G$ that are invariant under automorphisms of $G$. In symbols, if $H\\leq G$, and $\\varphi(H) = H \\ \\forall \\varphi \\in \\mathrm{Aut}(G)$, then $H$ is a characteristic subgroup of $G$.You don’t need to actually understand all these words, I’ll just note the important takeaways: Proposition 8. Characteristic subgroups Are normal subgroups, Make normal subgroups transitive. (1) tells us that characteristic subgroups are some stronger definition of normal subgroups. (2) works in a specific way. You likely found out that $K \\trianglelefteq H \\trianglelefteq G$, does not immediately imply that $K \\trianglelefteq G$. However, characteristic subgroups are a stronger form of normal subgroups. If $K \\trianglelefteq H \\trianglelefteq G$ and $K$ is a characteristic subgroup of $H$, then we can stretch to our conclusion $K \\trianglelefteq G$.For specific chains for solvable groups, we can show that these normal subgroups can be characteristic subgroups as well, and that allows us to reach as far as we want when claiming some group in the chain is normal to any other group in the chain. Problem 3 Prove that if $G$ is finite, then any normal Sylow $p$-subgroup of $G$ is a characteristic subgroup. Proof. $\\mathrm{Syl}_p(G)$ has all elements of order $p^n$ for all $n\\in \\mathbb{N}$. Since order is preserved in automorphisms, for any $a\\in \\mathrm{Syl}_p(G)$, we have $o(\\phi(a)) = p^k$ for some $k\\in\\mathbb{N}$, and therefore $\\phi(a)\\in\\mathrm{Syl}_p(G)$, so $\\phi(\\mathrm{Syl}_p(G))\\subseteq\\mathrm{Syl}_p(G)$. Since the order of the image of an automorphism is preserved, or $|\\phi(\\mathrm{Syl}_p(G))| = |\\mathrm{Syl}_p(G)|$, we have $\\phi(\\mathrm{Syl}_p(G))=\\mathrm{Syl}_p(G)$.Group actions on subgroups (and more)Since group actions move all elements of a set, we can do something clever. Every group element corresponds to a group action, and every group action corresponds to some permutation of the set $A$. Therefore, if the set is finite, we can create a homomorphism $\\varphi: G\\to S_{|A|}$ between $G$ and the symmetric group on $|A|$ letters. Since the kernel of a homomorphism is a normal subgroup of $G$, if we can show that $\\ker\\varphi$ is non-trivial, we can find a normal subgroup.If we let that set be the set of left cosets of some subgroup $H$ of $G$,and let the action be just multiplying the element to the left side of the coset (you can verify this is an action) then we can create the map $\\varphi: G\\to S_{[G:H]}$.There’s another action we can define, if we gather all the conjugate subgroups of $H$ and let $G$ act on them by conjugation, we create a map $\\varphi: G\\to S_{k}$, where $k$ is the number of conjugate subgroups to $H$.A more advanced application of group actions is by letting the action define a linear transformation on a vector space. This is the grounding idea of representation theory. Since matrices (linear transformations) have a stricter definition, and linear algebra in general is highly studied, we can reach a lot more conclusions from this connection.“Guaranteed” normal subgroupsThere are two types of subgroups that we can form for any group that are always normal in said group. Definition 9. The center of a group $G$ is the set of all elements that commute with all elements of $G$. It is denoted $Z(G)$ Definition 10. The commutator subgroup of $G$ is the set of all $xyx^{-1}y^{-1}$ for $x,y\\in G$. It is denoted $G’$.If these are trivial or the whole group, they can provide useful information too. If the center is the entire group, then the group is abelian.The commutator subgroups are useful since we can take the commutator of the commutator, $(G’)’$, and so on. If this eventually terminates at the trivial group $\\{1\\}$ (we call this series the derived series), we can show that the group $G$ is solvable.The isomorphism theoremsSee this link for their statements. Problem 4. Assume we know that $A_n$ is simple for $n\\geq 5$. Find all normal subgroups of $S_n$ for $n\\geq 5$. Proof. Let $H\\trianglelefteq S_n$ be any group other than $A_n$ normal to $S_n$.By the first isomorphism theorem, $H\\cap A_n \\trianglelefteq H$, so $H\\cap A_n = H \\implies H \\subseteq A_n$ or $H\\cap A_n=\\{1\\}$, since $H$ is normal. If $H \\subseteq A_n$, then by the second isomorphism theorem, $A_n/H \\trianglelefteq S_n/A_n\\cong \\mathbb{Z}_2$.So either $A_n/H$ is trivial, in which case $H=A_n$, or $H$ has index $2$ in $A_n$, which is not possible, since that would mean $A_n$ is not simple. Otherwise, if $H\\cap A_n=\\{1\\}$, $H$ has an odd permutation. Then $A_nH = S_n$. With these two facts, we conclude $S_n\\cong A_n\\times H$ and $H\\cong \\mathbb{Z}_2$. However, $S_n$ is not isomorphic to $A_n\\times\\mathbb{Z}_2$, since it has a non-trivial center, and all $S_n$ for $n\\geq 5$ have a trivial center.What about infinite groups?I came to realize while I finished this chapter that results were primarily applied to finite groups. This is because, in my opinion, the study of infinite groups has more on the horizon that of finite groups. This is not to discredit the work on finite groups, classifying all finite simple groups is probably one of the most monumental discoveries made in this field. However, if we want to keep pushing the boundaries of groups, infinite groups are just more interesting.I think that’s why geometric group theory has proven to be such a useful framework: it’s nearly useless for finite groups, but allows us to see a general structure (such as knowing about groups of finite index) for these infinite groups in which we can’t just find the Sylow $2$-subgroups." }, { "title": "My Abstract Algebra Notes + Thoughts on LaTeX Notes (~1/2 a year in)", "url": "/Aug24th/", "categories": "Notes", "tags": "Abstract Algebra", "date": "2022-08-24 15:49:00 -0500", "snippet": "My NotesSince I have a bit of time before I have to leave for college (Riju’s already left), I went and polished my Abstract Algebra notes from this year.You can find the PDF link to my notes here, and if you want the source code, you can find that here.These notes weren’t made live in class so there was less stress on me creating these notes quickly. I do think I’ll have some classes to try in-person LaTeX notes in the future though.The LaTeX JourneyI should say that these thoughts are from the perspective of someone who just finished high school, so my introduction to LaTeX wasn’t having to write a paper or anything.I started learning LaTeX through Evan Chen’s source code, which he leaves on his website. There are a lot of good templates there to see how he structures his .tex files. Also his LaTeX FAQ is quite useful (althougha lot of the setup isn’t necessary because in modern code editors, a lot of the compile process is handled already).I had two main reasons at the time for writing LaTeX: for my math notes, and for writing proofs for a local math competition.The latter definitely motivated me the most to learn LaTeX, because it meant that the PDFs I made had an audience.As much as die-hard TeX users complain about it, I started using Overleaf, which is alright for creating .tex files. If you are just taking notes, or writing something that isn’t going out to hundreds of people, I think Overleaf has enough to work as a LaTeX compiler. My course notes were mainly written in Overleaf.After that though, I did want to switch to something that could compile locally. As I was using Atom at the time (R.I.P.), I got a configuration working pretty quickly. Good things don’t last forever though, so I had to make the switch to VSCode. The TeX tools in VSCode are extremelycomprehensive, and seemed like a lot more than I needed, but they did get what I needed done relatively quickly. It is good that all you need is one package to get everything you need done.The FutureAs a whole, I would definitely recommend learning to write in LaTeX, since it is, in my opinion, the most efficient way to convey mathematics online, and to other people, other than handwriting of course. There are lots of online forums like Art of Problem Solving and Codeforces where learning to write in LaTeX is essential for writing any kind of solution,and I think that alone should be reason to learn to write basic mathematical expressions in it.I also can’t avoid the fact that LaTeX is just so pretty to write in. The aesthetics definitely won me over, especially looking at the types of notes others could make online, and being able to see my own writing become a stylized pdf with little theorem boxes :3.However, I know LaTeX has it limits, especially with diagrams. Now, to be fair, if I wanted to still write TeX files and create diagrams I could: Start learning different ways to efficiently insert hand-drawings or create diagrams to put into my pdfs, or Just have the TeX file for more organized and equation heavy math where I need to make sure that everything is in its right place, and a handwritten notebook for freeform thought, such as brainstorming a solution.I’ve seen a lot of arguments for both sides of this online, but I think that they all agreed that at least some form of handwritten notes is good to have. I agree with that. Sometimes spending too much time writing notes in a LaTeX doc demotivated me to just let the idea flow in my brain. It was harder to write in a disorganized way. Now, there aren’t a lot of reasons to want to be less organized, but sometimes a proof that I was writing would’ve worked easier as a quick sketch and a few arrows and words describing how the figure proves the point. While there are technically options for that, LaTeX emphasizes rigor through all the structure that it has.I did get the sense that having skills in LaTeX was not directly growing my mathematical skill, and I agree with that, but it was a lot of fun to learn and create using LaTeX this year. Modifying and improving my LaTeX setup made me feel better about what I knew, andall that really matters with notes though is writing something that makes you (and perhaps others) want to read it again and study it, and I think LaTeX notes did a pretty good job of that!" }, { "title": "Graph Ends and Their Purpose in Geometric Group Theory", "url": "/Jul6th/", "categories": "Research, Geometric Group Theory", "tags": "graphs, infinite graphs, ends, cayley graphs", "date": "2022-07-06 00:10:00 -0500", "snippet": "I’ll give an intuition for how mathematicians define what graph ends are, and explain their use in geometric group theory. However, before I can do that, I’ll explain what ends are in a graph theory perspective.What are ends?The word ends is a little confusing, because they don’t really represent the “end” of a graph, at least not how we would define an end in real life. Think of a rope. If two teams were playing tug of war on that rope, we know that it has two ends that the opposite teams would tug on. Now if this rope is of finite length (it probably is), then we would typically call the two points at the end of the rope the ends (obviously).However, knowing these “ends” isn’t that useful when talking about graphs that are infinite (which coincidentally are a lot of the interesting ones).So how would we define the ends of a graph that is infinite? Well the simplest thing to check is a straight, infinite line. We know it has two ends: to the left and to the right.Figure 1: An infinite lineWhat can we say about these ends? Well as we get further from the center of the line, they always get further from each other. What else? Well lets look at another graph.Figure 2: A binary treeI’m sure that you’ve come across a binary tree in your life, whether you realized it or not. The basic idea is that we start from the top, and we have two options of how to continue down. Then we have two more options from them, and the tree continues (I’m assuming this tree is infinite).What do we call an end on this? Well there are plenty of paths that go to infinity, (in fact, $2^{\\aleph_0}$ many) and they all certainly are distinct.Maybe a good starting idea is to cut the top vertex and the edges its attached to. Then we end up with two completely separated (disconnected), but infinite graphs. Can we conclude that there are only two ends? No! because that definitely contradicts what our intuition tells us about this graph.So let’s cut the next two vertices on the way down. Now we have four infinite graphs that are completely separated. As we continue, the number of separated graphs just gets bigger and bigger.Compare this to the number line, which, no matter how many edges we remove to “separate” the infinite ends, there always are two infinite ends (try to come up with some other way, it’s impossible).To formalize this, I need to introduce some more mathematical ideas about graphs.The formal definition of a graph $\\Gamma$ is a set a vertices $V(\\Gamma)$, and edges $E(\\Gamma)$ that connect the way you expect them to, edges connect vertices together. Additionally, the graph must be connected, meaning that there aren’t any vertices we can’t reach from any other vertex. Definition 1. The ball of size $D$ around a point $x_0 \\in \\Gamma$ consists of all edges that are a distance of $D$ or less away from $x_0$. We denote this set $B(x_0, D)$.Now for a graph $\\Gamma$, I want you to cut all edges in said ball around of size $D$ around the point $x_0$. The formal way to write this would be $\\Gamma \\setminus B(x_0,D)$. Now look at the number of infinite graphs that you just created by making this cut. As we make our cuts bigger and bigger, that number ends up becoming the number of ends on the graph!Sometimes, as I will show later, we can’t ever make a “cut” (of finite size) that is good enough to separate the graph into definite sets, but when we can, we call this an finite essential cutset.0 Ends: Any finite graphIt should be clear now that any finite graph has $0$ ends, because we definitely don’t have a way to take a finite graph and cut it into infinitely many graphs. Conversely, there definitely isn’t a way to split an infinite graph into a bunch of finite disconnected graphs with a finite number of cuts. So a graph has $0$ ends if and only if it is finite.2 Ends: The large (infinite) ropeLet’s make a cut on the rope:Figure 3: A rope cutIf we remove all the edges on that ball, then we end up with two infinite sets that aren’t connected to each other. It makes sense as we increase the radius of the ball, centered at the same point, we would still have two disconnected, infinite sets. Therefore, the infinite rope has two ends.Infinite Ends: The binary treeFigure 4: A binary tree cutRemoving every edge in the first ball gives us two disconnected, infinite sets, so there are two ends right? Well if we increase the size of the ball, the number of disconnected, infinite sets just keeps getting bigger, and it never really stops growing. In fact, the root of any disconnected set that hasn’t been cut off will result in two, newly created sets after it does get cut. Therefore, the (infinite) binary tree has infinite ends.One end: The coordinate latticeThis one should build the idea the most in your head, but it may not seem intuitive: the lattice graph has only one end.Figure 5: The lattice graphWhat? you say, but the lattice is a bunch of ropes attached to each other, why would they not have infinitely many ends? Well notice that as we increase the size of our ball, we can always create a path around the ball from every vertex outside the ball to each other, meaning that they are all connected. So really all the “ends” of this graph are connected to each other, and we only have one end.One additional note to help ease any concerns: Proposition 2. The point that we choose to center the ball cut ($x_0$) does not matter; as we approach infinity, the same number of ends will reveal themselves.Applications to Geometric Group TheoryGroups can be represented geometrically/graphically by Cayley Graphs, which take the generators of the group as directed edges, and the elements of the group as vertices. Proposition 3. The number of ends in the Cayley graphs of two groups does not change if the groups are quasi-isometric.Quasi-isometry is one of the most important principles in geometric group theory, so having this invariant is very useful. Note: this doesn’t mean that two groups are quasi-isometric if they have the same amount of edges (a.k.a. the converse is not true), but it does mean that we can rule out two groups being quasi-isometric if they do not have the same number of edges on their Cayley graph. Additionally, the problem of counting ends simplifies a lot for group Cayley graphs: Proposition 4. A Cayley grpah has either $0, 1, 2,$ or $\\infty$ ends.I won’t give a concrete proof here, but note that if a graph has $3$ ends (the same applies for any other finite value $\\geq 3$), then we can show that we can move the ball that separates the group into multiple ends into infinitely many places (through the fact that a group can act on its own Cayley graph). This means that when we can show that the number of ends are greater than $2$, we know that the graph has infinitely many ends.Finally, to finish off, there is something useful we can get for one of the end values: Theorem 5. If $\\Gamma = \\mathbf{Cay}_{A}(G)$, the Cayley graph for a group $G$ with generators $A$, and $\\lvert \\mathrm{Ends}(\\Gamma) \\rvert = 2$, then $G$ has a finite index subgroup isomorphic to $\\mathbb{Z}$.Obviously this is true for $\\mathbf{Cay}(\\mathbb{Z})$ (which has two ends), because the group is $\\mathbb{Z}$. However, the power in this theorem that if there are two ends to a Cayley graph, then we automatically know a property of a finite index subgroup, and gain more information about the structure of the larger group." }, { "title": "What is a basis? It&#39;s complicated...", "url": "/Jul4th/", "categories": "Fun", "tags": "topology, free groups, linalg", "date": "2022-07-04 14:06:00 -0500", "snippet": "Simply put, a basis in mathematics is the simplest way to represent something, be it a set, object, anything! However, in looking into how several fields of mathematics define a basis, it can turn out very different, and it is entirely dependent on the mathematical idea that the field is studying. So lets start with the basis that most entry-level mathematicians should be familiar with.Basis in Linear AlgebraLinear algebra is based on the idea of a vector space, which has two components: a field of sclalars and a set of vectors $\\mathbb{V}$. Vector spaces must be closed under scalar multiplication, which means that we can multiply any scalar by any vector and define it as scaling up. They are also closed under vector addition, so the sum of any two vectors should also be in the vector space.The typical introduction to what a basis is in linear algebra is that any linear combination of the set of bases uniquely creates every single element of the vector space. Notice how linear combinations make use of the fact that we have scalar multiplication and vector addition. Without that, we would be screwed, because there would be no way for our scalars and vectors to interact!There’s also one more fact about the basis of a vector space; all the vectors have to be linearly independent. For vectors to be linearly independent, there should be no way to get one vector from a linear combination of the others. Example 1. It should feel obvious that $\\begin{bmatrix}1 \\ 0\\end{bmatrix}$ and $\\begin{bmatrix}0 \\ 1\\end{bmatrix}$ are linearly independent. Formalizing that “obvious” feeling is part of the job of linear algebra.The reason we mention this is because when using a basis, we generally want to have as few of them as possible. Why? Because if we weren’t seeking the minimum, then we could have arbitrarily large bases. The minimum number of bases of a vector space doesn’t change. This type of value that doesn’t change, for example, you always need $3$, no less, no more, to define $\\mathbb{R}^3$, also known as space. This property is called an invariant, and means that we can use it for really useful concepts like dimension, which simply is the number of bases needed to define the vector space. It would be weird to say that a dimension of $\\mathbb{R}^3$ is $3$, does that mean there are others? No, we know that the dimension of $\\mathbb{R}^3$ is $3$.Basis in TopologyI’ll give an overview of what a topology and a topological space are. Definition 1. Given a set $X$, a topology is a set $\\mathcal{T}$ that contains subsets of $X$.As a side note, anytime I use $\\mathcal{THIS}$ $\\mathcal{FONT}$ when talking about a set, it means a set that contains sets within it, like a meta-set, or, more commonly said, a collection of sets. Continuing on… $\\mathcal{T}$ must satisfy the following properties: The empty set and the whole set $X$ must both be in $\\mathcal{T}$ The collection is closed under arbitrarily many unions. The collection is closed under finitely many intersections. To clear up the second and third requirement, the second says that if we take any number of sets in $\\mathcal{T}$ and find the union of them, they should also be in $\\mathcal{T}$. The third statement says that the same applies for intersections, but only finitely many (this is to avoid some bad contradictions).We call $(X,\\mathcal{T})$ a topological space is $\\mathcal{T}$ is a topology on $X$. Here’s an example to build intuition: Example 2. Define the particular point topology on a set $X$ with an element $p \\in X$ as follows:\\[\\mathcal{T}_p := \\{ U \\subseteq X \\mid p \\in U \\}\\cup \\{ \\emptyset\\}.\\] Or put simply, the set of all subsets of $X$ that have $p$ in them. We also have to include the empty set due to the definition of a topology. Exercise. Show this is a topology.So what makes a basis on this type of space? Well if we have a basis, we should be able to form every element in the topology just by using the basis and the properties that a topological space has. Definition 2. A basis of a topology $\\mathcal{B}$ is a collection of subsets of $X$ such that $\\mathcal{B}$ covers $X$ Given $B_1, B_2 \\in \\mathcal{B}$, for all $x \\in B_1 \\cap B_2$, there is $B_x \\in \\mathcal{B}$ such that $x \\in B_x \\subseteq B_1 \\cap B_2$. The reason we need the first requirement should be obvious. The topology contains $X$, the full set, so if $\\mathcal{B}$ did not cover all of $X$, then we could not make a union and get the full set $X$.The second requirement says given two subsets in $\\mathcal{B}$, for all elements $x$ in the intersection of those subsets, there is another basis element that is contained in the intersection and has $x$. The reason that we need this is not entirely clear, but I encourage looking up proofs for why this is required to have a topology (hint: it mainly has to do with the intersection property of topologies).Bases in topology don’t really have this size factor that linear algebra has, mainly because the bases of topologies tend to be infinite. However, they can be used in other ways to show, for example, that one topology is contained in the other (can you guess how?).“Basis” in Abstract AlgebraThis summer I’ve been doing research work on geometric group theory, which required me to learn about combinatorial group theory. I’m going to explain the basic motivations that led mathematicians to choose to continue research in those fields. As a reminder, a group is defined as follows: Definition 3. A set $S$ of elements is a group if there is: A binary operation $\\cdot$ that takes two elements of the set and sends the to another element: $s_1\\cdot s_2 = s_3 \\in S$. Associativity, meaning that $(a\\cdot b)\\cdot c = a\\cdot (b\\cdot c)$. Basically, we can ignore parenthesis; the order that we do things doesn’t matter! The identity is an element called $1_G$ such that any element combined with $1_G$ gives back that element. Every element has inverses, such that $a\\cdot a^{-1} = a^{-1}\\cdot a = 1_G$. To help build the idea of a “basis” in a group, I’ll start with a simpler concept, cyclic groups.Cyclic GroupsThe idea with a cyclic group is to choose one element of the group, say $a$, and let it interact with only itself (and its inverse). We call this the cyclic group, denoted $\\langle a \\rangle$, or the group generated by $a$. What does it look like?, kind of like this:\\[\\langle a \\rangle = \\{ \\cdots, a^{-2}, a^{-1}, 1_G, a, a^2, \\cdots\\}.\\]Imagine putting $a$ in a box and shaking it until we had all possible products of $a$ (and $a^{-1}$, which we know exists by the definition of a group).Now I’ll ask a weird question: how could we make it finite? Say we wanted to make the group only have $7$ elements just for the kick of it. Well how about this: once we get to $a^7$, we tell the group to stop and turn around back to $1_G$, the identity. What about the negative values? Well notice that\\[a^{-2} = a^{-2}(1_G) = a^{-2}(a^7) = a^5,\\]so really we can just call $a^{-2}$, $a^5$. We can call this “rule” $a^7=1_G$. What we’ve just created is called a relator on $G$,and $a$ is called a generator of $G$. We can combine these to get the presentation of $G$. For example, we would denote the group we created above as\\[G = \\langle a \\mid a^7 = 1_G \\rangle = \\{ 1_G, a, a^2, \\cdots, a^6\\}.\\]Presentation of GroupsIn fact, all groups have a presentation. We give them some generators and then add some “rules” (or relators) that they must follow (aside from the regular rules of all groups), and then it can be used to represent any group!This is the best form of creating a “basis” for a group. Example 3. Here’s a presentation for $\\mathbb{Z}^2$, or all 2D points that have integer coordinates:\\[\\mathbb{Z}^2= \\langle a = \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}b= \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\mid ab=ba\\rangle.\\] I should note that $ab = ba$ is actually a necessary relator we need to mention. See, nowhere in the group definition does it say that groups elements have to commute. This rule says they do. Groups that also commute have a special name: Abelian Groups.Alien MathFigure 1: GlepThe alien mathematician Glep comes up to you, and explains how he also knows group theory (which the aliens call Glepglop theory in honor of him) and created his definition of $\\mathbb{Z}^2$ (which he calls $\\overline{\\overline{\\mathfrak{REAL}}}$),\\[\\overline{\\overline{\\mathfrak{REAL}}}= \\langle a = \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}b= \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\mid ab=ba\\rangle.\\]Well there’s nothing wrong! This works just as fine as the other definition. So now suppose that we weren’t told that both of these groups create $\\mathbb{Z}^2$. How would we know that Glep came from a different cultural background and was creating the same group in a different way? How do we know\\[\\langle a = \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}b= \\begin{bmatrix} 99 \\\\ 1 \\end{bmatrix} \\mid ab=ba\\rangle,\\]also works? (check it, it does). While it may seem easy to tell for an easy group like $\\mathbb{Z}^2$, groups can get very complex (citation), and we’ve found there is no general rule to telling us that two representations are the same. However, we can get pretty close. Our means of telling the Glep that we really are talking about the same group drove study of both combinatorial and geometric group theory." }, { "title": "DeMorgan&#39;s Law", "url": "/Jun28th/", "categories": "Set Theory", "tags": "set theory, proofs", "date": "2022-06-28 10:00:00 -0500", "snippet": "DeMorgan’s Laws are one of the most important laws in basic set theory and come up time and time again. The proof of DeMorgan’s law provides a nice introduction to proofs and reasoning to why it works. Theorem. $(A \\cup B)^c = A^c \\cap B^c$Proof.\\[\\begin{align*}&amp;amp;\\text{First prove that } (A \\cup B)^c \\subseteq A^c \\cap B^c \\\\&amp;amp;\\text{Let } x \\in (A \\cup B)^c. \\\\&amp;amp;x \\notin A \\text{ and } x \\notin B. \\\\&amp;amp;x \\in A^c \\text{ and } x \\in B^c. \\\\&amp;amp;x \\in A^c \\cap B^c \\\\\\end{align*}\\]Now we have to prove the statement in the opposite direction to establish equality.\\[\\begin{align*}&amp;amp;\\text{Now prove that } A^c \\cap B^c \\subseteq (A \\cup B)^c \\\\&amp;amp;\\text{Let } x \\in A^c \\cap B^c. \\\\&amp;amp;x \\in A^c \\text{ and } x \\in B^c. \\\\&amp;amp;x \\notin A \\text{ and } x \\notin B. \\\\&amp;amp;x \\in (A \\cup B)^c \\\\&amp;amp;\\therefore (A \\cup B)^c = A^c \\cap B^c \\\\\\end{align*}\\]As you can see, most of the work is simply just breaking apart the set theory language, interpreting it and putting it back together in a new way. Given this proof, the next exercise should be fairly simple. Exercise. Prove $(A \\cap B)^c = A^c \\cup B^c$" }, { "title": "Number Theory - Perfect Numbers", "url": "/Jun19th/", "categories": "Number Theory", "tags": "number theory, perfect numbers", "date": "2022-06-19 10:00:00 -0500", "snippet": "Perfect numbers are a highly studied subset of the positive integersthat relate to our understanding of number theory.I will show a proof of the structure of all even perfect numbers,and a bonus section on the conspiracy of odd perfect numbers.$\\sigma(n)$ will denote the sum of divisors of $n$, a positive integer. Definition 1. If $n$ factors into prime factors $n = p_1^{e_1}p_2^{e_2}\\cdots p_t^{e_t}$, then\\[\\sigma(n)=\\prod_{i=1}^{t}(1+p_i+p_i^2+\\cdots+p_i^{a_i}) = \\prod_{i=1}^{t}\\frac{p_i^{a_i+1}-1}{p_i-1}\\] (the last simplification is a result of this). Definition 2. A number $n$ is perfect if $\\sigma(n)=2n$.Notice that $\\sigma(n)$ is multiplicative, meaning that for positiveintegers $m$ and $n$, if their greatest common divisor is $1$, then $\\sigma(mn)=\\sigma(m)\\sigma(n)$.Creating Perfect Numbers Theorem 3. An even number is perfect if and only if it is of the form $2^{p-1}(2^p-1)$, where $p$ is a prime and $2^p-1$ is a prime.Proof.To prove this, we can just use the formula for $\\sigma(n)$. Note that $n$ is already in its factored form, as we assumed$2^p-1$ is a prime. Therefore,\\[\\begin{align*} \\sigma(n) &amp;amp;= \\frac{2^{p-1+1}-1}{2-1}\\cdot\\frac{(2^p-1)^{2}-1}{(2^p-1)-1} \\\\&amp;amp;= (1+2^{p}-1)\\cdot 2^p-1\\\\&amp;amp;= 2^p(2^p-1) \\\\&amp;amp;= 2(2^{p-1}(2^p-1)) \\\\&amp;amp;= 2n.\\end{align*}\\]As desired. More interestingly, the converse of the claim is also true. That is, all even perfect numbers must be of the form $2^{p-1}(2^p-1)$.To show this, we will use a proof by contrapositive. Assume that $p$is prime, but $2^p-1$ is not. As a consequence of $\\sigma$ being multiplicative, we can write\\[\\begin{align*}\\sigma(2^{p-1}(2^p-1)) &amp;amp;= \\sigma(2^{p-1})\\sigma(2^p-1) \\\\&amp;amp;= 2^p\\cdot\\sigma(2^p-1)\\end{align*}\\]We aren’t actually going to calculate $\\sigma(2^p-1)$, we just need to have a bound that shows that it cannot be a perfect number anymore.Notice that $\\sigma(2^p-1)$ not being prime means that it has more thantwo factors. It has factors $1,2^p-1$ (all numbers do), and has somemysterious other factor $q &amp;gt; 1$. However, the sum of these is $\\sigma(2^p-1) \\geq 1+2^p-1+q = 2^p+q$. Plugging this back into the equation,\\[\\begin{align*}2n=\\sigma(2^{p-1}(2^p-1)) &amp;amp;= 2^p\\cdot\\sigma(2^p-1) \\\\ &amp;amp;= 2^p\\cdot(2^p+q) \\\\ 2n = 2^{p}(2^p-1) &amp;amp;&amp;lt; 2^p\\cdot(2^p+q)\\end{align*}\\]So the sum of divisors will always be more than $2n$. This means that thenumber cannot be perfect, finishing the proof.$\\blacksquare$So we can create as many as we want right?Probably not, or possibly yes. There are two conditions for generatingthese perfect numbers. $p$ must be prime. There exist infinitely many primes (credit to Euclid), so we don’t have to worry about limitations with finding prime numbers. However, $2^p-1$ also being prime is an issue. These numbers are called the Mersenne primes. The thing is, it’s an unsolved problem as to whether or not there are an unlimited amount of them.The Mersenne prime condition limits whether or not we can prove there are infinitely many perfect numbers.Bonus: Odd Perfect Numbers?An open problem in math is whether odd perfect numbers exist or not. While we have not proof they don’t exist, numbers have been checked to very high valueswith no perfect odd numbers.In fact, there are a lot of properties (that Iwould recommend checking out, they get very specific) that an odd perfectnumber must satisfy, but we still have yet to find an example or a proof showing that it does not exist." }, { "title": "Fields - Finite Fields", "url": "/Jun15th/", "categories": "Abstract Algebra", "tags": "fields, algebra, splitting fields", "date": "2022-06-15 10:00:00 -0500", "snippet": "With groups, it’s easy for us to find finite ones that have the same order,but are not the same (up to isomorphism). The simplest example I can give is$\\mathbb{Z}_4$ and $\\mathbb{Z}_2\\times\\mathbb{Z}_2$, which both have order $2$, but they are not isomorphic. The former is a cyclic group, but the latter is not!However, with fields, I hope to show in this post that fields are the exactly the same as each other (up to isomorphism)if they have the same finite order. This means when I tell you I have a field of order $4$, I really mean the field of order $4$–there is only one.Splitting FieldsI do not want to spend too much time focusing on splitting fields, but I will mention some important takeaways that are useful for this proof. Definition 1. A splitting field is a field such that for a polynomial$f(x)$ over a field $K$, all roots are contained in the field.For example, the splitting field for $r(x)=x^2-2$ over $\\mathbb{Q}$ is $\\mathbb{Q}(\\sqrt{2})$, since the roots of this function are $\\pm\\sqrt2$,meaning we must attach $\\sqrt2$ to $\\mathbb{Q}$. Theorem 2.Let $f(x)$ be a polynomial over $K$. If the extensions fields $F$ and $E$are both splitting fields over $K$, then they are isomorphic.Proof. An entire chapter of the book. See these notes for a full proof. $\\blacksquare$Size of Finite Fields Definition 3. The characteristic of a field $F$ is the smallestpositive number $n$ such that $1\\cdot n =0$. If one does not exist,the characteric is $0$. Denoted $\\mathrm{char}(F)$.All integral domains have characteristic $0$ or $p$, where $p$ is a prime. Since all fields are integral domains, we know that fields can only have characteristic $0$ or $p$. Theorem 4.All finite fields with characteristic $p$ have order $p^n$, where $p$ is a prime number, and $n$ is a positive integer.Proof. First note that if a field $F$ has characteristic $p$,then the homomorphism $\\phi: \\mathbb{Z}\\rightarrow F$ such that$\\phi(n)=n\\cdot1$ has a kernel $p\\mathbb{Z}$. From the ring isomorphism theorem,we can say that the image of $\\phi(\\mathbb{Z})$, which we will call$K$, is isomorphic to $\\mathbb{Z}/\\ker(\\phi) = \\mathbb{Z}/p\\mathbb{Z} \\cong \\mathbb{Z}_p$.Now, since $F$ is a vector space over $K$, and both $F$ and $K$are finite, we let $[F:K]=n$. Every element $f \\in F$ has the unique form\\[f=a_1v_1+a_2v_2+\\cdots+a_nv_n\\]for a basis $v_i$ and coefficients $a_i$, with $a_i \\in K$. To count the number of possible elements we can form, simply use combinatorics. There are $p$ choices for each $a_i$, because there are $p$ elements in $K$. So the number of possible elements is $p^n$, for the $n$ coefficients. $\\blacksquare$We call $K$ the prime subfield of $F$. Note that $1 \\in K$.This theorem gives us an idea about how some fields are structured.Useful Theorems/Lemmas Theorem 5. Let $F$ be a finite field with $p^n$ elements. Then $F$ is a splitting field for $x^{p^n}-x$ over its prime subfield $K$. Corollary 6. Two finite fields are isomorphic if and only if they have the same number of elements.Proof. Let $F$ and $E$ both have $p^n$ elements, and have prime subfields $K$ and $L$. $K\\cong L\\cong \\mathbb{Z}_p$. Sincethe polynomial $x^{p^n}-x$ splits over $F$ and $E$ in $K$ and $L$,$F$ and $E$ are both splitting fields, and by Theorem 2, they areisomorphic. $\\blacksquare$ Lemma 7. Let $F$ be a field with characteristic $p$, and let $n$be a positive integer.\\[\\{ a \\in F \\mid a^{p^n}-a=0\\}\\] is a subfield of $F$. Lemma 8. Let $F$ be a field of characteristic $p$. If $n$ is apositive integer not divisible by $p$, then $x^n-1$ has no repeatedroots in any extension field of $F$.The Main Theorem Theorem 9. For prime $p$ and positive integer $n$, there is a field with $p^n$ elements.Proof. We want to create a field $F$ with $p^n$ elements. So let$F$ be the splitting field for $f(x)=x^{p^n}-x$ over $\\mathbb{Z}_p$.Since\\[f(x)=x^{p^n}-x = x(x^{p^n-1}-1),\\]and $p$ does not divide $p^n-1$, we know from Lemma 8 that there areno repeated roots in the polynomial. Lemma 7 tells us that allthese roots form their own field. Therefore $F$ is a field that contains all roots of $f(x)$, of which there are $p^n$ unique ones.$\\blacksquare$Since any finite field of the same order is isomorphic, and we can create fields of any possible order $p^n$, we can call these finite fieldsthe field of order $9,27,100,729$ etc. In fact, the field with $p^n$ elements is called the Galois fieldof order $p^n$, denoted $\\mathrm{GF}(p^n)$.The main point in all of this is that we have a fairly good understanding of how fields can behave when they are finite–somethingthat we can’t say for even finite groups (see Monster group).This makes finite fields a good candidate for studying cryptography,where encryption needs to be hard to crack, but easy to implement." }, { "title": "Fields - Extensions and Degrees", "url": "/Jun9th/", "categories": "Abstract Algebra", "tags": "fields, algebra, linalg", "date": "2022-06-09 10:00:00 -0500", "snippet": "Field Extensions and Linear AlgebraI hope to show the relationship between field extensions and linearalgebra, and how a proof using the latter can extend to auseful property of the former.Extension Fields are Just Vector SpacesAn extension field of a field $F$ is any field that contains $F$as a subfield. There are many ways to extend a field. If you takean element $u \\notin F$, and then let $u$ be in $F$,then it extends $F$. We denote this $F(u)$, and this is a fieldextension. Since we only added a single element,this is called a simple extension.Turns out there is a fundamental connection between field extensions,and linear algebra, and can be used to prove some interesting theorems.Most importantly,If $F$ is a field extension of $K$ then $F$ is simplya vector space over $K$.Elements of $K$ are scalars, and elements of $F$ are vectors.D(imensions)egreesWe can also recontextualize dimension in terms of our fields, sincesticking with linear algebra terms can get confusing when dealing withgroups because of the different contexts (sorry Riju).The degree of $F$ as a vector space over $K$is denoted $[F:K]$. This has the exact same meaning as dimension,but can be used to come to different conclusions from the eyes of groups. Theorem 1: Multiplicativity Formula for Degrees Let $E$ be an field extension of $K$ and $F$be a field extension of $E$. Then, \\([F:K] = [F:E][E:K]\\)The real interesting part of this for me (and why I’m writing thisin the first place) is the fact that the proof uses basic conceptsfrom linear algebra to prove this.Proof.Since we are dealing with field extensions, $F$ is just a vector spaceover $E$, and $E$ is just a vector space over $K$.Therefore we can come up with a basis ${u_1, u_2, \\dots, u_n}$ for$F$ over $E$ (assuming $[F:E]=n$), and the basis${v_1,v_2,\\dots,v_m}$ for $E$ over $K$ (assuming $[E:K]=m$).I claim that the set of pairwise products, $u_iv_j$, whichI will call the set $\\mathcal{B}$ is the basis forthe vector space of $F$ over $K$. We can do this by showing that$\\mathcal{B}$ both spans $F$ over $K$ and each basis is linearlyindependent. The number of ways tocombine $u$’s and $v$’s is $nm$, which would complete the proof.To show that $\\mathcal{B}$ spans $F$ over $K$, note that for any element $u$in $F$, we can have\\[u= \\sum_{i=1}^{n}a_iu_i,\\]where each $u_i$ is the basis for $F$ over $E$, and each $a_i$ isin $E$, since $F$ is a vector space over $E$. Similarly,for each element $a_i$ in $E$, we can write it as\\[a_i = \\sum_{j=1}^mc_{ij}v_j,\\]where each $v_j$ is the basis for $E$ over $K$, and each $c_{ij}$ isin $K$. Well then we can combine these sums to get\\[u = \\sum_{i=1}^n\\sum_{j=1}^mc_{ij}v_ju_i,\\]showing us that any $u$ in $F$ can be found as the sum of its basis vectorstimes $c_{ij}$ in $K$.The next step is to show that $\\mathcal{B}$ is linearly independent.Recall that a set is linearly independent if any linear combination ofthe set that equals $0$ implies that all coefficients are $0$ as well.Suppose $ \\sum_{i,j}c_{ij}v_ju_i=0$ for a linear combination ofthe set $\\mathcal{B}$. This is the same as\\[0 =\\sum_{i=1}^n\\left(\\sum_{j=1}^mc_{ij}v_j\\right)u_i.\\]Notice that the inner “$j$” sum is a coefficient for the $u_i$ basis.Now since each $u_i$ form a basis for $F$ over $E$, each of itscoefficients $\\sum_{j=1}^mc_{ij}v_j$ must be $0$. Similarly, each $v_j$form a basis for $E$ over $K$, so each coefficient $c_{ij}$must also be $0$. This completes the proof, because we have shown thata linear combination of $\\mathcal{B}$ that equals $0$ must have allcoefficients $c_{ij}$ equal $0$.$\\blacksquare$Now what use does this linear algebra proof have beyond linear algebraitself? Here’s a useful example for how we can use it to determine thedegree of one field extension over the original field. Example 2: $[\\mathbb{Q}(\\sqrt3,\\sqrt2):\\mathbb{Q}]$ From Theorem 1, we know that\\[[\\mathbb{Q}(\\sqrt3,\\sqrt2):\\mathbb{Q}] = [\\mathbb{Q}(\\sqrt3,\\sqrt2):\\mathbb{Q}(\\sqrt2)][\\mathbb{Q}(\\sqrt2):\\mathbb{Q}].\\] We know that $[\\mathbb{Q}(\\sqrt2):\\mathbb{Q}]=2$, because the minimalpolynomial for $\\sqrt2$ in $\\mathbb{Q}$ is $p(x)=x^2-2$. Moreover, wecan show that the minimal polynomial for $\\sqrt3$ in $\\mathbb{Q}(\\sqrt2)$is $x^2-3$. Therefore $[\\mathbb{Q}(\\sqrt3,\\sqrt2):\\mathbb{Q}(\\sqrt2)]=2$.So\\[[\\mathbb{Q}(\\sqrt3,\\sqrt2):\\mathbb{Q}] = 2\\cdot 2= \\boxed{4}.\\] What’s more, we can use this to find the basis for$\\mathbb{Q}(\\sqrt3,\\sqrt2)$ over $\\mathbb{Q}$, it is a field extension and therefore a vector space. The basis $\\mathbb{Q}(\\sqrt2)$ over $\\mathbb{Q}$ is ${1,\\sqrt2}$,and the basis for $\\mathbb{Q}(\\sqrt3,\\sqrt2)$ over $\\mathbb{Q}(\\sqrt2)$is ${1,\\sqrt{3}}$, so taking all possible products, the basis is\\[\\{ 1, \\sqrt2, \\sqrt3, \\sqrt6\\},\\] perfectly matching up with our degree, $4$.The idea of a basis, as shown by the last example, is very differentfrom the basis used in a typical introduction class for Linear Algebra.We don’t use vectors as a basis, instead we just use numbers. Itbecomes much more abstract it that way.It should be clear that this theorem is very useful for understandinghow field extensions interact, and allow us to learn more aboutthe degrees of certain field extensions. I hope to showthe further use of this theorem in future posts." }, { "title": "Fields - Algebraic Elements", "url": "/Jun8th/", "categories": "Abstract Algebra", "tags": "fields, transcendentals", "date": "2022-06-08 07:01:26 -0500", "snippet": "Algebraic ElementsWhich number is “further” from the integers, $\\sqrt{2}+\\sqrt{3}$ or$\\sqrt{2}$? It intuitively feels like it would be harder toget to $\\sqrt2+\\sqrt3$ using the integers and the operations we know,like squarerooting, adding, and subtracting. But is there any quantifiableway to explain those intuitions?This post is to help show what makes a number algebraic.To do this, I need to explain what fields are, and then we canexpand the idea to show that some numbers, like $\\sqrt{2}+\\sqrt{3}$ are“harder” to get than $\\sqrt{2}$ from the integers, $\\mathbb{Z}$.What are Fields?Fields are a unique typeof algebraic structure. They are a lot like agroup, but instead of justhaving one operation, they have two. “Addition”, and “Multiplication”.I only put those in quotes because technically they don’t have tobehave like the good old addition and multiplication we know,but for the most part, we can assume that they are.Along with this, fields have distributivity. Itsays that, \\(a(b+c) = ab + ac,\\) which means that addition andmultiplication have a means of interacting with each other.We define addition and division the way we expect them to be, and we have them act on our regular numbers,whether they be $\\mathbb{Z}$ or $\\mathbb{R}$.Algebraic Numbers and DegreesNow when we say a number is algebraic in a certain field, we meanthat some polynomials with coefficients in that field has a rootthat is the number we are looking for. Example $\\sqrt{2}$ is algebraic in $\\mathbb{Z}$. Why?Because if $x=\\sqrt2$, then $x^2=2$, so a polynomial that we can formwith $\\sqrt2$ as a root is $f(x)=x^2-2$.In fact, in the last example, it’s easy to see that $x^2-2$ doesn’tsplit into any smaller polynomials. This means that $f(x)$ isan irreducible polynomial. In reality, all algebraic numbers of a fieldhave a unique irreducible polynomial that has it as a root.This fact allows us to get a scale for how algebraic a number can be.We see that the irreducible polynomial that has a root of $\\sqrt2$ hasa degree (largest power of $x$) of $2$. This motivates us to definethe degree of $\\sqrt2$ over $\\mathbb{Z}$ as $2$.Now think about the beginning question, what polynomial has a root of$\\sqrt2+\\sqrt3$? Well a good starting point, like the last one,was to let $x=\\sqrt2+\\sqrt3$, and try to create a polynomial withinteger roots out of it. The process is as follows:\\[\\begin{align*}x &amp;amp;= \\sqrt2+\\sqrt3 \\\\x- \\sqrt2 &amp;amp;= \\sqrt3 \\\\x^2-2\\sqrt2x + 2 &amp;amp;= 3 \\\\x^2 -1 &amp;amp;= 2\\sqrt2x \\\\x^4-10x^2+1&amp;amp;=0\\end{align*}\\]Our desired polynomial is $x^4-10x^2+1$. Checking that thisis irreducible is important, but I won’t cover it here,(see Einsenstein’s Irreducibility Criterion). Once we know it is irreducible,then we can conclude that $\\sqrt2+\\sqrt3$ has degree $4$ in $\\mathbb{Z}$.Well then, since we need a higher degree to find a polynomial with a root of$\\sqrt2+\\sqrt3$, there is a mathematical justification for why it is“harder” to get $\\sqrt2+\\sqrt3$ over $\\sqrt2$.Bonus: TranscendentalsTranscendental numbers are often discussed when talking about $\\pi$ or $e$.The definition of transcendental number is simple: a number is transcendentalin a field $F$ if it isn’t algebraic. What that’s saying is that there isno polynomial that exists that has $\\pi$ or $e$ as a root. Proofs for theseare quite difficult, but a google search may be worth the time to look at them." }, { "title": "Linear Algebra - Playing with Markdown and LaTeX", "url": "/Feb15th/", "categories": "Fun, Linear Algebra", "tags": "", "date": "2022-02-15 06:01:26 -0600", "snippet": "LlamasHello! This is my first real post, so this is going to mostly just be a test of my Markdown abilities.Anyways I thought it was interesting how repetitive the introduction of linear algebra can be. That’s not to say that the repetition is bad, it’s just that we’ve found a lot of ways to essentially say the same thing.Consider a system of $m$ equations with $n$ variables:\\[\\begin{gather*}a_{11}x_1 + a_{12}x_2 + a_{13}x_3 + \\cdots + a_{1m}x_n = b_1 \\\\a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + \\cdots + a_{2m}x_n = b_2 \\\\a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + \\cdots + a_{3m}x_n = b_3 \\\\\\vdots \\\\a_{m1}x_1 + a_{m2}x_2 + a_{m3}x_3 + \\cdots + a_{mn}x_n = b_m\\end{gather*}\\]Even if we had no idea how to do Linear Algebra, we could probably solve for $x_1, x_2, x_3 \\cdots$. (provided that a solution exists, of course). We would just use a combination of elimination and substitution.Augmented MatricesBut mathematicians got lazy and were looking for easier way to solve them. So they took all the coefficients ($a_1, a_2, a_3 \\cdots$) of the variables ($x_1, x_2, x_3, \\cdots$) in the equations and made a matrix, like so:\\[\\begin{equation*}A_{m,n} =\\begin{bmatrix}a_{11} &amp;amp; a_{12} &amp;amp; \\cdots &amp;amp; a_{1n} \\\\a_{21} &amp;amp; a_{22} &amp;amp; \\cdots &amp;amp; a_{2n} \\\\\\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots \\\\a_{m1} &amp;amp; a_{m2} &amp;amp; \\cdots &amp;amp; a_{mn}\\end{bmatrix}\\end{equation*}\\]And then they just appended the numbers $b_1, b_2, b_3$ to the end. We’ve obtained what is known as the augmented matrix!\\[\\begin{equation*}A_{m,n} =\\begin{bmatrix}a_{11} &amp;amp; a_{12} &amp;amp; \\cdots &amp;amp; a_{1n} &amp;amp; b_{1}\\\\a_{21} &amp;amp; a_{22} &amp;amp; \\cdots &amp;amp; a_{2n} &amp;amp; b_{2}\\\\\\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots &amp;amp; \\vdots \\\\a_{m1} &amp;amp; a_{m2} &amp;amp; \\cdots &amp;amp; a_{mn} &amp;amp; b_{m}\\end{bmatrix}\\end{equation*}\\]Now you can do a bunch of row operations and solve the equations.Vector EquationsIn fact, the augmented matrix and vector equations are exactly equivalent forms! You can prove it yourself, with some very straightforward expansion.\\[\\begin{align*}x_1\\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1}\\end{bmatrix} +x_2\\begin{bmatrix} a_{12} \\\\ \\vdots \\\\ a_{m2}\\end{bmatrix} +x_3\\begin{bmatrix} a_{1m} \\\\ \\vdots \\\\ a_{mn}\\end{bmatrix} =\\begin{bmatrix} b_{1} \\\\ \\vdots \\\\ b_{m}\\end{bmatrix}\\end{align*}\\]Matrix-Vector MultiplicationAnd all this, in fact, is equal to matrix-vector multiplication. I won’t get into the long proof here because it’s annoying to write out and boring. If you know matrix-vector multiplication you should be able to prove it for yourself.\\[\\begin{align*}A\\vec{x} = \\vec{b}\\end{align*}\\]A is the coefficient matrix, $\\vec{x}$ is a column vector of all your variables and $\\vec{b}$ stores the solutions to each equation. Linear systems of equations, augmented matrices, and vector equations and matrix-vector multiplication are all basically the same thing. Alright, that’s all for now. No new thoughts, just figuring out how to write.Cheers." }, { "title": "Blog Post 02/12/22 - Math League #5 Write-ups", "url": "/Feb12th/", "categories": "Math League", "tags": "sfft, logs, diophantine, quadratic", "date": "2022-02-12 11:18:26 -0600", "snippet": "Math League #5Some problems from the Wisconsin Math Leauge. I scored 6/6! :DRiju and I agreed that it was probably the easiest one so far this year,though the last one still requires some thinking.I waited 24 hours to post this, and I hope that is a good grace period for the problems. Problem 3What $n \\in \\mathbb{Z}$ satisfies\\[n &amp;lt; \\frac{1}{\\log_{\\frac{1}{2}}\\frac{1}{3}} + \\frac{1}{\\log_{\\frac{1}{5}}\\frac{1}{3}} &amp;lt; n+1 ?\\]While this is really just a change in base exercise and then plugging it into a calculator for most people,we can still bound it without a calculator and get the answer. I’ll assume you have gotten to this point,\\[n &amp;lt; \\frac{\\log{\\frac{1}{10}}}{\\log{\\frac{1}{3}}} &amp;lt; n+1.\\]Set the base equal to $\\frac{1}{3}$ to make the bottom part dissapear:\\[n &amp;lt; \\log_{\\frac{1}{3}}{\\frac{1}{10}} &amp;lt;n+1.\\]Past this all you need to know is that $\\log_{\\frac{1}{3}}{\\frac{1}{10}}$ is somewhere between$\\log_{\\frac{1}{3}}{\\frac{1}{9}} =2$ and $\\log_{\\frac{1}{3}}{\\frac{1}{27}} =3$, so $n=2$. We’re done. Problem 6Which rational numbers $r$ give the quadratic\\[rx^{2} + (r-2)x+(r+2) = 0\\] two integer roots?We start by letting the two integer roots be $p,q$. After expanding $(x-p)(x-q)$, we find that\\[x^2 + (-q-p)x +qp = 0.\\]We make the coefficient of the first equation $1$ by dividing it by $r$\\[x^2 + \\frac{r-2}{r}x + \\frac{r+2}{r} = 0.\\]So we want the coefficents of these two equations to be the same, yeilding\\[-q-p = \\frac{r-2}{r} \\qquad\\qquad qp = \\frac{r+2}{r}\\]The key point here is that we can solve both equations for $2$, hopefully removing the constants.\\[(p+q+1)r = 2 \\qquad\\qquad (qp-1)r = 2.\\]Then finally setting these equal to each other, and dividing $r$, which is indeed possible because $r\\neq 0 $:\\[p+q+1 = qp-1.\\]The $p$, $q$ and $qp$ along with the fact that we are finding integer solutions screamed Simon’s favoritefactoring trick.\\[\\begin{align*}0 &amp;amp;= qp - p - q - 2 \\\\0 &amp;amp;= (q-1)(p-1)-1-2 \\\\3 &amp;amp; = (q-1)(p-1).\\end{align*}\\]The solution is so close… we notice that the only way to get $3$ with integers is $(1,3),(3,1),(-1,-3)(-3,-1)$(the last few negative ones almost messed me up because I missed them). Also note that the switching thepair does not do anything, as $q$ and $p$ will just swap, which has no affect on our value of $r$. $p-1$ $q-1$ $1$ $3$ $-1$ $-3$ Solving for $p$, and $q$ (which I will leave as an exercise for the reader), we plug each value back into$ (p+q+1)r = 2 $ from before to get our two values of $r$: $\\boxed{-2, \\frac{2}{7}}$." }, { "title": "Blog Post 02/05/22 - AIME Mock Reflections", "url": "/Feb5th/", "categories": "", "tags": "", "date": "2022-02-05 11:18:26 -0600", "snippet": "My First AIME Mocks!I have been very inactive in this diary. I invested most of my timein my notes on Real Analysis and Modern Algebra.I’ll try to publish them soon if I think they are up to standard.I participated in two different mock AIMEs today, so it’s safeto say that my brain is pretty fried. I won’t post any write-ups right nowbecause neither are finished, but I will link the contests.The first one I did in the morning was theIMTC contest.15 questions with 3 hours. I answered 7 of them,and guessed on 2 more. The first few were quite easy and caused myconfidence to go up until the last few questions, which killed me.The second contest was the TTLM contest, which I started in the evening. I was more relaxed doing this,I did not even plan on using paper for anything other than the geoproblems, so I stuck to my whiteboard. I answered 6, and guessed on 1more. Again, I started to get stuck on some problems. I will also notethat the time on this exam was 2 hours compared to the normaltime period of 3 hours. The questions however seemed easier,so it seemed to balance out the difficulty of this exam.LaTeX to Markdown Test ProblemHow many positive integers $n \\leq 242$ exist such that the remainderswhen $n$ is divided by $1, 2, 11, 22, 121$ and $242$, respectively, forma non-decreasing sequence?Denote $n_k$ as taking $n \\bmod{k}$. Our series becomes\\(0, n_2, n_{11}, n_{22}, n_{121}, n\\) We can simply compare consecutiveterms, and as long as the next one is greater than or equal to theprevious one, we are good.It is easy to verify that $n_2 \\geq 0, n_{22} \\geq n_{11},$ and$n_{121} \\geq n$ are always true. That leaves us to find examples of$n_{11}&amp;lt;n_2$ and $n_{121}&amp;lt;n_{22}$.When $n=11$, \\(n_2=1 &amp;gt; 0 = n_{11}.\\) Past this, $n_{11}$ continues toincrease, while $n_2$ stays at $0$ or $1$. Even at $n=22$,$n_2 = 0 = 0 = n_{11}$. But when $n=33$, $n_2 = 1 &amp;gt; 0 = n_{11}$. Weconclude that when $n \\equiv 11 \\pmod{22}$, we have $n_{11}&amp;lt;n_2$. Thisis also easy to verify.For $n_{121}&amp;lt;n_{22}$, when $n&amp;lt;121$, $n_22$ remains small, while$n_{121}$ continues to increase, so all of these numbers should be good.When $n=121,$ we have \\(n_{22} = 11 &amp;gt; 0 = n_{121}.\\) $n_{22}$ continuesto increase at the same pace of $n_{121}$ until $n=132$, where\\(n_{22} = 0 &amp;lt; 11 = n_{121}.\\) Past this, $n_{121}$ continues toincrease until $n=242$, which is where we would have stopped anyway. Condition Is false… $n_2 \\geq 0$ Never $n_{11} \\geq n_2$ $n \\equiv 11\\bmod{22}$ $n_{22} \\geq n_{11}$ Never $n_{121} \\geq n_{22}$ $121 \\leq n \\leq 131$ $n_{121} \\geq n$ Never There are $11$ $n$ that satisfy$n \\equiv 11\\bmod{22}$, and $11$ $n$ that satisfy $121 \\leq n \\leq 131$.However, don’t forget PIE, the number $121$ is counted twice! Thereforethe $n$ that satisfy the conditions are\\(242 - (11 + 11 - 1) = \\boxed{221}.\\)" } ]
